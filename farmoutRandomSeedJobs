#!/bin/bash
#
# Setup:

#
# Usage:
#       farmoutRandomSeedJobs <jobName> <nEvents> <nEventsPerJob> <CMSSW Version> <config file>
#       Expects a cfi called IOMC/GeneratorInterface/data/PythiaSource<jobName>.cfi to exist
#       in the CMSSW area that you are using.
#  Example: farmoutCmsRunJobs blah 1000000 100 ~/CMSSW_1_2_0 /path/to/my.cfg
#
# The config file may refer to the following macros, which are automatically
# inserted by this script:
#
# $randomNumber
# $randomNumber2
# $nEventsPerJob
# $outputFileName

# Initialize default settings:

PNFS_HOME=/pnfs/hep.wisc.edu/data5/uscms01/$USER
SRM_SERVER=srm://cmssrm.hep.wisc.edu:8443
DCAP_SERVER=dcap://cmsdcap.hep.wisc.edu:22125
SRM_HOME=${SRM_SERVER}/${PNFS_HOME}
DCAP_HOME=${DCAP_SERVER}${PNFS_HOME}

DISK_REQUIREMENTS=2000
MEMORY_REQUIREMENTS=900
MIN_PROXY_HOURS=24

# special exit status to force job to leave the queue
FAIL_JOB=42

scratch_dir="/data"
if ! [ -d $scratch_dir ]; then
  scratch_dir="/scratch"
fi
if ! [ -d $scratch_dir ]; then
  scratch_dir="/tmp"
fi
SUBMIT_HOME=${scratch_dir}/$USER

logerror() {
  echo 2>&1 "$@"
}

die() {
  logerror
  logerror "$@"
  exit 1
}

outputFileExists() {
  fname=$1

  #Strip off srm://hostname:8443 to get raw pnfs path.
  local_fname=${fname#srm://*:8443}

  if [ -f "$local_fname" ]; then
    return 0
  fi
  return 1
}

check_proxy() {
  hours=$1
  proxy=$2
  if ! [ -f "$proxy" ]; then
    die "NOTE: No grid proxy found.  (Expecting to find it here: $proxy.)"
  fi

  #Issue a warning if less than this many seconds remain:
  min_proxy_lifetime=$((3600*$hours))

  seconds_left="`voms-proxy-info --timeleft --file=$proxy 2>/dev/null`"

  if [ "$seconds_left" = "" ]; then
    echo "WARNING: cannot find time remaining for grid proxy."
    voms-proxy-info -all -path $proxy
    return 0
  fi
  if [ "$seconds_left" -lt "$min_proxy_lifetime" ]; then
    logerror
    logerror "NOTE: grid proxy is about to expire:"
    logerror "voms-proxy-info"
    voms-proxy-info --file=$proxy
    return 1
  fi

}

PrintUsage() {
  echo "USAGE: farmoutRandomSeedJobs [options] <jobName> <events> <events-per-job> <CMSSW Path> <config file>"
  echo ""
  echo "OPTIONS:"
  echo "  --output-dir=${SRM_HOME}/<jobName>"
  echo "  --submit-dir=${SUBMIT_HOME}/<jobName>"
  echo "  --no-submit"
  echo "  --skip-existing-output  (do not create job if output file exists)"
  echo "  --skip-existing-jobs    (do not create job if already created)"
  echo "  --memory-requirements=$MEMORY_REQUIREMENTS (megabytes)"
  echo "  --disk-requirements=$DISK_REQUIREMENTS  (megabytes)"
  echo "  --save-failed-datafiles  (save root file from failed cmsRun job)"
  echo "                           (in <output-dir>-cmsRun-failed)"
  echo ""
  exit 2
}

OPTS=`getopt -o "h" -l "help,output-dir:,submit-dir:,no-submit,skip-existing-output,skip-existing-jobs,disk-requirements:,memory-requirements:,save-failed-datafiles" -- "$@"`
if [ $? -ne 0 ]; then PrintUsage; fi

eval set -- "$OPTS"

NO_SUBMIT=
OUTPUT_DIR=
SUBMIT_DIR=
SKIP_EXISTING_OUTPUT=
SKIP_EXISTING_JOBS=
SAVE_FAILED_DATAFILES=

while [ ! -z "$1" ]
do
  case "$1" in
    -h) PrintUsage;;
    --help) PrintUsage;;
    --no-submit) NO_SUBMIT=1;;
    --output-dir) shift; OUTPUT_DIR=$1;;
    --submit-dir) shift; SUBMIT_DIR=$1;;
    --skip-existing-output) SKIP_EXISTING_OUTPUT=1;;
    --skip-existing-jobs) SKIP_EXISTING_JOBS=1;;
    --disk-requirements) shift; DISK_REQUIREMENTS=$1;;
    --memory-requirements) shift; MEMORY_REQUIREMENTS=$1;;
    --save-failed-datafiles) SAVE_FAILED_DATAFILES=1;;
    --) shift; break;;
    *) die "Unexpected option $1";;
  esac
  shift
done

if [ "$#" -ne 5 ]; then PrintUsage; fi

# Check for some required utilities
for exe in scramv1 condor_submit cmsRun.sh realpath voms-proxy-info; do
  if ! which $exe >& /dev/null; then
    die "Cannot find $exe in PATH.  Your environment is not correctly set up."
  fi
done

# Additional command-line arguments

jobName=$1
declare -i nEvents=$2
declare -i nEventsPerJob=$3
CMSSW_HOME=`realpath $4`
configTemplate=`realpath $5`

#       Ensure that your environment is correct

if ! [ -d "$CMSSW_HOME" ]; then
  die "No such directory: $CMSSW_HOME"
fi

proxy=${X509_USER_PROXY:-/tmp/x509up_u$UID}

if [ "$NO_SUBMIT" != 1 ] && ! check_proxy $MIN_PROXY_HOURS $proxy; then
  logerror
  logerror "Either rerun this command with --no-submit or create a new grid proxy"
  logerror "and rerun this command.  Example of how to create a grid proxy:"
  logerror
  logerror "voms-proxy-init --voms=cms --hours=48"
  exit 1
fi

# Check the config template

for macro in \$randomNumber \$nEventsPerJob \$outputFileName; do
  if ! grep -F -q $macro $configTemplate; then
    die "$macro must appear on the configuration template.  I can't find it in $configTemplate"
  fi
done

# Note: reverse sort order is _very_ important, or the search/replace
# operation will do the long thing on longer macros having a common
# prefix with a shorter macro.
randomMacros=`grep -o '\$randomNumber[0-9]*' $configTemplate | sort -r | uniq`

#
# Environment setup
#
originalDir=`pwd`
PATH=$PATH:$originalDir
export PATH
cd $CMSSW_HOME || die "Failed to cd to $CMSSW_HOME."
eval `scramv1 runtime -sh`

if [ "$?" != "0" ]; then
  die "Failed to initialize CMSSW environment with scram in $CMSSW_HOME."
fi

OUTPUT_DIR=${OUTPUT_DIR:-${SRM_HOME}/$jobName}
SUBMIT_DIR=${SUBMIT_DIR:-${SUBMIT_HOME}/$jobName}
submitFile=$SUBMIT_DIR/submit

if [ -d "$SUBMIT_DIR" ] && [ "$SKIP_EXISTING_JOBS" != "1" ]; then
  logerror
  logerror "Error: Submit directory already exists: $SUBMIT_DIR"
  logerror
  logerror "You must either remove it, or specify --skip-existing-jobs, or"
  logerror "specify a different job name or submission directory with --submit-dir"
  exit 1
fi

mkdir -p $SUBMIT_DIR
cd $SUBMIT_DIR || die "Failed to create directory $SUBMIT_DIR"


#
# Job specification
#
Executable=`which cmsRun.sh`

#
# CMS Dashboard parameters
#
FARMOUT_DASHBOARD_REPORTER=`which farmout_dashboard.sh 2>/dev/null`
if [ "$FARMOUT_DASHBOARD_REPORTER" = "" ]; then
   echo "No farmout_dashboard.sh found, so no reporting to the CMS dashboard."
fi
if [ "$CMS_DASHBOARD_REPORTER" = "" ]; then
   echo "No CMS_DASHBOARD_REPORTER defined, so no reporting to the CMS dashboard."
fi
dboard="
dboard_taskId=${USER}-`hostname -f`-\$(Cluster)
dboard_jobId=\$(Process)
dboard_sid=\$\$([GlobalJobId])
dboard_application=`basename ${CMSSW_HOME}`
dboard_exe=cmsRun
dboard_tool=farmout
dboard_scheduler=local-condor
dboard_taskType=simulation
dboard_broker=local-condor-`hostname -f`
dboard_user=${USER}
dboard_SyncCE=${CMS_DASHBOARD_LOCAL_CE}
CMS_DASHBOARD_REPORTER=${CMS_DASHBOARD_REPORTER}
FARMOUT_DASHBOARD_REPORTER=${FARMOUT_DASHBOARD_REPORTER}
"
# convert newlines to spaces
dboard="`echo $dboard`"

if [ "$SAVE_FAILED_DATAFILES" != "" ]; then
  #cmsRun.sh checks for this in the environment
  save_failed_datafiles_env="SAVE_FAILED_DATAFILES=1"
fi

# First put all the submit file commands that are the same for all jobs.
    cat <<EOF > $submitFile
X509UserProxy        = ${proxy}
Universe             = vanilla
Executable           = $Executable
GetEnv               = true
Environment          = "${dboard} ${save_failed_datafiles_env}"
Copy_To_Spool        = false
Notification         = never
WhenToTransferOutput = On_Exit
on_exit_remove       = (ExitBySignal == FALSE && (ExitCode == 0 || (ExitCode == ${FAIL_JOB} && JobRunCount>3)))
ImageSize            = $(($MEMORY_REQUIREMENTS*1000))
+DiskUsage           = $(($DISK_REQUIREMENTS*1000))
Requirements = TARGET.HasAFS_OSG =?= True && TARGET.OSRedHatRelease =!= "Scientific Linux SL Release 3.0.4 (SL)"
EOF


#
# Starting values for the job loop
#
declare -i nEventsSubmitted=0
declare -i job=0
#
# Loop over jobs
#
while (( $nEvents > $nEventsSubmitted )); do
#
# Name the files
#
    jobtag=$jobName-`printf "%4.4d" $job`

    let job=$job+1
    if [ "$SKIP_EXISTING_JOBS" = "1" ] && [ -d $jobtag ]; then
      continue
    fi

    if [ "$SKIP_EXISTING_OUTPUT" = "1" ]; then
      # Check for existing output file
      if outputFileExists $OUTPUT_DIR/$outputFileName; then
        continue
      fi
    fi

    conlog=$jobtag.log
    stdout=$jobtag.out
    stderr=$jobtag.err
    jobcfg=$jobtag.cfg
    outputFileName=$jobtag.root

    randomSed=""
    # Note: we rely here upon the reverse sort order of the randomMacros
    # in order to assure proper treatment of macros that share a common
    # prefix (e.g. randomNumber11 should be replaced before randomNumber1)
    for randomMacro in $randomMacros; do
        randomNumber=`date +%N%S | cut -c 1-6,10- | sed 's|^0*||'`
        randomSed="${randomSed}s/\\${randomMacro}/${randomNumber}/g;"
    done

#
# Create the job subdirectory
#
    mkdir -p $jobtag || die "Failed to mkdir $jobtag"

#
# Prepare job configuration file
#

sed < $configTemplate \
  "${randomSed}
   s|\\\$nEventsPerJob|$nEventsPerJob|g;
   s|\\\$outputFileName|$outputFileName|g" > $jobtag/$jobcfg

#
# Prepare condor submit commands for the job
#
    cat <<EOF >> $submitFile

InitialDir           = $jobtag
Arguments            = $jobcfg $outputFileName $OUTPUT_DIR
Transfer_Input_Files = $jobcfg
output               = $stdout
error                = $stderr
Log                  = $conlog
Queue
EOF
#
# Prepare for the next job
#
    let nEventsSubmitted=$nEventsSubmitted+$nEventsPerJob

done

#
# Submit the jobs
#
if [ -z "$NO_SUBMIT" ]; then
    # The job is messed up if X509_USER_PROXY is defined, because then
    # Condor doesn't override this value to point to the actual proxy
    # location on the execution node.
    unset X509_USER_PROXY

    condor_submit $submitFile
else
    echo "Submit file $submitFile has been created but not submitted."
fi

echo "Jobs for $nEventsSubmitted events of $jobName are created in $SUBMIT_DIR"

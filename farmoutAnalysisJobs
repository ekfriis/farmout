#!/bin/bash
#
# Setup:

#
# Usage:
#       farmoutAnalysisJobs <jobName> <CMSSW Version> <config file>
#
# The config file should refer to the following macros, which are automatically
# inserted by this script:
#
# $inputFileNames     ==>  Will be replaced by list of input files
# $outputFileName     ==>  Will be replaced by the $inputFileName-output.root
#
# Job parameters
#


# Initialize default settings:

# for storing output
SRM_SERVER=srm://cmssrm.hep.wisc.edu:8443
PNFS_HOME=/pnfs/hep.wisc.edu/store/user/$USER

# for getting input
DCAP_SERVER=dcap://cmsdcap.hep.wisc.edu:22125
# path to directory containing lfns: /store/...
PNFS_STORE=/pnfs/hep.wisc.edu

SRM_HOME=${SRM_SERVER}${PNFS_HOME}
DCAP_HOME=${DCAP_SERVER}${PNFS_HOME}

SITE_REQUIREMENTS='TARGET.HasAFS_OSG =?= True && TARGET.OSRedHatRelease =!= "Scientific Linux SL Release 3.0.4 (SL)" && IsSlowSlot=!=true'
DISK_REQUIREMENTS=2000
MEMORY_REQUIREMENTS=900
HOLD_IMAGE_SIZE_FACTOR=4.0
HOLD_DISK_USAGE_FACTOR=10.0
MIN_PROXY_HOURS=24

# We want explicit references to "app_rw" to be replaced with "app"
# so the interactive osg app volume is not pounded by the jobs.
SITE_ENV_MATCH_REPLACE_1='s|/afs/hep.wisc.edu/osg/app_[^/]*/|/afs/hep.wisc.edu/osg/app/|g'

# special exit status to force job to leave the queue
FAIL_JOB=42

for scratch_dir in /data /scratch /tmp; do
  if [ -d $scratch_dir ] && [ -w $scratch_dir ]; then
    break
  fi
done
SUBMIT_HOME=${scratch_dir}/$USER

basename() {
  # This shell function is faster than calling /bin/basename
  path=$1
  suffix=$2
  path=${path##*/}  # get everything after the final '/'
  if [ ! -z $suffix ]; then
    path=${path%$suffix}
  fi
  echo $path
}

realpath() {
  if ! [ -a "$1" ]; then
    echo "$1"
  fi
  readlink -f $1
}

logerror() {
  echo 2>&1 "$@"
}

die() {
  if [ $# -gt 0 ]; then
    logerror
    logerror "$@"
  fi
  exit 1
}

outputFileExists() {
  fname=$1

  #Strip off srm://hostname:8443 to get raw pnfs path.
  local_fname=${fname#srm://*:8443}

  if [ -f "$local_fname" ]; then
    return 0
  fi
  return 1
}

check_proxy() {
  hours=$1
  proxy=$2
  if ! [ -f "$proxy" ]; then
    logerror
    logerror "NOTE: No grid proxy found.  (Expecting to find it here: $proxy.)"
    return 1
  fi

  #Issue a warning if less than this many seconds remain:
  min_proxy_lifetime=$((3600*$hours))

  seconds_left="`voms-proxy-info --timeleft --file=$proxy 2>/dev/null`"

  if [ "$seconds_left" = "" ]; then
    echo "WARNING: cannot find time remaining for grid proxy."
    voms-proxy-info -all -path $proxy
    return 0
  fi
  if [ "$seconds_left" -lt "$min_proxy_lifetime" ]; then
    logerror
    logerror "NOTE: grid proxy is about to expire:"
    logerror "voms-proxy-info"
    voms-proxy-info --file=$proxy
    return 1
  fi

}

PrintUsage() {
  echo "USAGE: farmoutAnalysisJobs [options] <jobName> <CMSSW Path> <config file>"
  echo ""
  echo "OPTIONS:"
  echo "  --output-dir=${SRM_HOME}/<jobName>-<runName>"
  echo "  --input-dir=${DCAP_HOME}/<jobName>"
  echo "  --input-dbs-path=/dbs/dataset/path   (in place of --input-dir)"
  echo "  --input-runs=Run1,Run2,...           (must be used with --input-dbs-path)"
  echo "  --submit-dir=${SUBMIT_HOME}/<jobName>-<runName>"
  echo "  --no-submit"
  echo "  --job-count=N            (limit the number of jobs that are created)"
  echo "  --input-files-per-job=1"
  echo "  --skip-existing-output   (do not create jobs if output file exists)"
  echo "  --skip-existing-jobs     (do not create jobs if job already created)"
  echo "  --match-input-files='*.root'"
  echo "  --exclude-input-files=pattern"
  echo "  --input-file-list=file"
  echo "  --memory-requirements=$MEMORY_REQUIREMENTS (megabytes)"
  echo "  --disk-requirements=$DISK_REQUIREMENTS   (megabytes)"
  echo "  --site-requirements=$SITE_REQUIREMENTS"
  echo "  --save-failed-datafiles  (save root file from failed cmsRun job)"
  echo "                           (in <output-dir>-cmsRun-failed)"
  echo "  --save-missing-input-file-list=file"
  echo "  --assume-input-files-exist (do not check file existence at submit time)"
  echo "  --quick-test             (require low latency scheduling)"
  echo "  --extra-inputs=file1,file2,...  (e.g. parameter files)"
  echo "  --accounting-group=cms_name  (username to use for fair-sharing)"
  echo "  --requires-whole-machine (job should reserve all slots on machine)"
  echo "  --fwklite                (in this case, provide script instead of config)"
  echo "  --output-files-per-subdir=N (0 for infinite)"
  echo ""
  echo "Note that <runName> is taken from the name of the config file."
  exit 2
}

OPTS=`getopt -o "h" -l "help,output-dir:,input-dir:,submit-dir:,no-submit,job-count:,skip-existing-output,skip-existing-jobs,match-input-files:,exclude-input-files:,input-files-per-job:,disk-requirements:,memory-requirements:,input-file-list:,input-dbs-path:,input-runs:,save-failed-datafiles,save-missing-input-file-list:,assume-input-files-exist,site-requirements:,quick-test,extra-inputs:,accounting-group:,requires-whole-machine,fwklite,output-files-per-subdir:" -- "$@"`
if [ $? -ne 0 ]; then PrintUsage; fi

eval set -- "$OPTS"

ACCOUNTING_GROUP=
NO_SUBMIT=
JOB_LIMIT=
SKIP_EXISTING_OUTPUT=
SKIP_EXISTING_JOBS=
OUTPUT_DIR=
INPUT_DIR=
SUBMIT_DIR=
MATCH_INPUT_FILES='*.root'
EXCLUDE_INPUT_FILES=
INPUT_FILES_PER_JOB=1
INPUT_FILE_LIST=
INPUT_DBS_PATH=
INPUT_RUNS=
SAVE_FAILED_DATAFILES=
SAVE_MISSING_INPUT_FILE_LIST=
ASSUME_INPUT_FILES_EXIST=
CFG_EXTENSION=
QUICK_TEST=
EXTRA_INPUTS=
REQUIRES_WHOLE_MACHINE=
FWKLITE=
APPLICATION=cmsRun
OUTPUT_FILES_PER_SUBDIR=0


while [ ! -z "$1" ]
do
  case "$1" in
    -h) PrintUsage;;
    --help) PrintUsage;;
    --no-submit) NO_SUBMIT=1;;
    --job-count) shift; JOB_LIMIT=$1;;
    --skip-existing-output) SKIP_EXISTING_OUTPUT=1;;
    --skip-existing-jobs) SKIP_EXISTING_JOBS=1;;
    --output-dir) shift; OUTPUT_DIR=$1;;
    --input-dir) shift; INPUT_DIR=$1;;
    --submit-dir) shift; SUBMIT_DIR=$1;;
    --match-input-files) shift; MATCH_INPUT_FILES=$1;;
    --exclude-input-files) shift; EXCLUDE_INPUT_FILES=$1;;
    --input-files-per-job) shift; INPUT_FILES_PER_JOB=$1;;
    --disk-requirements) shift; DISK_REQUIREMENTS=$1;;
    --memory-requirements) shift; MEMORY_REQUIREMENTS=$1;;
    --input-file-list) shift; INPUT_FILE_LIST=$1;;
    --input-dbs-path) shift; INPUT_DBS_PATH=$1;;
    --input-runs) shift; INPUT_RUNS=$1;;
    --save-failed-datafiles) SAVE_FAILED_DATAFILES=1;;
    --save-missing-input-file-list) shift; SAVE_MISSING_INPUT_FILE_LIST=$1;;
    --assume-input-files-exist) ASSUME_INPUT_FILES_EXIST=0;;
    --site-requirements) shift; SITE_REQUIREMENTS="$1";;
    --quick-test) QUICK_TEST=1;;
    --extra-inputs) shift; EXTRA_INPUTS="$1";;
    --accounting-group) shift; ACCOUNTING_GROUP="+AccountingGroup=\"$1\"";;
    --requires-whole-machine) REQUIRES_WHOLE_MACHINE=1;;
    --fwklite) FWKLITE=1; APPLICATION=fwklite;;
    --output-files-per-subdir) shift; OUTPUT_FILES_PER_SUBDIR=$1;;
    --) shift; break;;
    *) die "Unexpected option $1";;
  esac
  shift
done

# Check for some required utilities
for exe in scramv1 condor_submit cmsRun.sh readlink voms-proxy-info ; do
  if ! which $exe >& /dev/null; then
    die "Cannot find $exe in PATH.  Your environment is not correctly set up."
  fi
done

if [ "$#" -ne 3 ]; then PrintUsage; fi


# Additional command-line arguments

jobName=$1
CMSSW_HOME=`realpath $2`
configTemplate=`realpath $3`

# Now we have all the user's input.

if [ "$EXTRA_INPUTS" != "" ]; then
  for file in ${EXTRA_INPUTS//,/ }; do
    if ! [ -a $file ]; then
      logerror "ERROR: Cannot find file specified in --extra-inputs: $file"
      die
    fi
  done
fi

# Check the config template

if [ "$configTemplate" = "" ]; then
  die "You must specify a cfg template."
fi

CFG_EXTENSION="${configTemplate/*./}"

if [ "$FWKLITE" != 1 ]; then
  if [ "$CFG_EXTENSION" != "py" ] && [ "$CFG_EXTENSION" != "cfg" ]; then
    die "cfg template must end in .py or .cfg"
  fi

  for macro in \$inputFileNames \$outputFileName; do
    if ! grep -F -q $macro $configTemplate; then
      die "$macro must appear on the configuration template.  I can't find it in $configTemplate"
    fi
  done
fi


runName=`basename $configTemplate .${CFG_EXTENSION}`

if [ "$INPUT_DIR" = "" ]; then
    if [ "$INPUT_FILE_LIST" != "" ] || [ "$INPUT_DBS_PATH" != "" ]; then
        INPUT_DIR=${DCAP_SERVER}${PNFS_STORE}
    fi
fi

OUTPUT_DIR=${OUTPUT_DIR:-${SRM_HOME}/$jobName-$runName}
INPUT_DIR=${INPUT_DIR:-${DCAP_HOME}/$jobName}
SUBMIT_DIR=${SUBMIT_DIR:-${SUBMIT_HOME}/$jobName-$runName}

#Strip off dcap://hostname:22125 to get raw pnfs path.
LOCAL_INPUT_DIR=${INPUT_DIR#dcap://*:22125}

#Get the part of INPUT_DIR that was stripped off.
INPUT_BASE=${INPUT_DIR%$LOCAL_INPUT_DIR}

if [ "$INPUT_FILE_LIST" != "" ]; then
    if ! [ -f "$INPUT_FILE_LIST" ]; then
        die "Error: No such file: $INPUT_FILE_LIST"
    fi
    INPUT_FILE_LIST=`realpath $INPUT_FILE_LIST`
elif ! [ -d "$LOCAL_INPUT_DIR" ]; then
  die "Error: No such input directory: $LOCAL_INPUT_DIR"
fi

if [ "$INPUT_DBS_PATH" != "" ]; then
    if [ "$DBSCMD_HOME" = "" ] || ! [ -d "$DBSCMD_HOME" ]; then
      die "DBS client is not correctly set up: DBSCMD_HOME is invalid"
    fi
    if [ "$INPUT_FILE_LIST" != "" ]; then
      die "--input-dbs-path cannot be used with --input-file-list"
    fi
fi

if [ "$INPUT_RUNS" != "" ]; then
  if [ "$INPUT_DBS_PATH" = "" ]; then
    die "--input-runs requires --input-dbs-path to be specified"
  fi
fi

if ! [ -d "$CMSSW_HOME" ]; then
  die "Error: No such CMSSW directory: $CMSSW_HOME"
fi

if [ -d "$SUBMIT_DIR" ] && [ "$SKIP_EXISTING_JOBS" != "1" ]; then
  logerror
  logerror "Error: Submit directory already exists: $SUBMIT_DIR"
  logerror
  logerror "You must either remove it, or specify --skip-existing-jobs, or"
  logerror "specify a different job name or submission directory with --submit-dir"
  die
fi

proxy=${X509_USER_PROXY:-/tmp/x509up_u$UID}

if [ "$NO_SUBMIT" != 1 ] && ! check_proxy $MIN_PROXY_HOURS $proxy; then
  logerror
  logerror "Either rerun this command with --no-submit or create a new grid proxy"
  logerror "and rerun this command.  Example of how to create a grid proxy:"
  logerror
  logerror "voms-proxy-init --voms=cms --hours=48"
  die
fi


#
# CMSSW environment setup
#
originalDir=`pwd`
PATH=$PATH:$originalDir
export PATH
cd $CMSSW_HOME || die "Failed to cd to $CMSSW_HOME."
eval `scramv1 runtime -sh`

if [ "$?" != "0" ]; then
  die "Failed to initialize CMSSW environment with scram in $CMSSW_HOME."
fi

for mr_name in ${!SITE_ENV_MATCH_REPLACE*}; do
    mr="${!mr_name}"
    # apply search-replace to the environment
    eval `env | sed "$mr" | sed 's/"/\\\\"/g' | sed 's/\\([^=]*\\)=\\(.*\\)/export \\1="\\2"/'`
done

runDir=$SUBMIT_DIR
submitFile=$runDir/submit
farmoutLogFile=$runDir/farmoutAnalysisJobs.log

# Make sure submitFile name is unique in case we are continuing a previous
# submission.
if [ -f $submitFile ]; then
  num=1
  while [ -f $submitFile.$num ]; do
    num=$(($num+1))
  done
  submitFile=$submitFile.$num
fi


mkdir -p $runDir

cd $runDir || die "Failed to create directory $runDir"

#
# Job specification
#
Executable=`which cmsRun.sh`

if [ "$QUICK_TEST" != "" ]; then
  SITE_REQUIREMENTS="${SITE_REQUIREMENTS} && IsFastQueueSlot =?= True"
fi

#
# CMS Dashboard parameters
#
FARMOUT_DASHBOARD_REPORTER=`which farmout_dashboard.sh 2>/dev/null`
if [ "$FARMOUT_DASHBOARD_REPORTER" = "" ]; then
   echo "No farmout_dashboard.sh found, so no reporting to the CMS dashboard."
fi
if [ "$CMS_DASHBOARD_REPORTER" = "" ]; then
   echo "No CMS_DASHBOARD_REPORTER defined, so no reporting to the CMS dashboard."
fi
if [ "$INPUT_DBS_PATH" != "" ]; then
    dboard_datasetFull="dboard_datasetFull=${INPUT_DBS_PATH}"
fi
dboard="
dboard_taskId=${USER}-`hostname -f`-\$(Cluster)
dboard_jobId=\$(Process)
dboard_sid=\$\$([GlobalJobId])
${dboard_datasetFull}
dboard_application=`basename ${CMSSW_HOME}`
dboard_exe=${APPLICATION}
dboard_tool=farmout
dboard_scheduler=local-condor
dboard_taskType=analysis
dboard_broker=local-condor-`hostname -f`
dboard_user=${USER}
dboard_SyncCE=${CMS_DASHBOARD_LOCAL_CE}
CMS_DASHBOARD_REPORTER=${CMS_DASHBOARD_REPORTER}
FARMOUT_DASHBOARD_REPORTER=${FARMOUT_DASHBOARD_REPORTER}
"
# convert newlines to spaces
dboard="`echo $dboard`"

if [ "$SAVE_FAILED_DATAFILES" != "" ]; then
  #cmsRun.sh checks for this in the environment
  save_failed_datafiles_env="SAVE_FAILED_DATAFILES=1"
fi

if [ "$FWKLITE" = 1 ]; then
  # The "configTemplate" in this case is actually the fwklite script.

  # This causes it to run the fwklite script rather than running
  # cmsRun.
  fwklite_env="FWKLITE_SCRIPT=$(basename ${configTemplate})"

  # add the fwklite script to the input files to be transferred to the job
  if [ "$EXTRA_INPUTS" != "" ]; then
    EXTRA_INPUTS="${EXTRA_INPUTS},"
  fi
  EXTRA_INPUTS="${EXTRA_INPUTS}${configTemplate}"
fi

if [ "$REQUIRES_WHOLE_MACHINE" == "1" ]; then
  REQUIRES_WHOLE_MACHINE_ATTR="+RequiresWholeMachine=true"

  #Note: the reference to per-slot Memory and Disk below are to prevent
  #condor from inserting the default requirements for these.  Instead,
  #we want to look at whole machine attributes.
  SITE_REQUIREMENTS="${SITE_REQUIREMENTS} && TARGET.CAN_RUN_WHOLE_MACHINE && TARGET.TotalMemory*1024 >= MY.ImageSize && TARGET.Memory > 0 && TARGET.TotalDisk >= MY.DiskUsage && TARGET.Disk > 0"
fi

# First put all the submit file commands that are the same for all jobs.
    cat <<EOF > $submitFile
X509UserProxy        = ${proxy}
Universe             = vanilla
Executable           = $Executable
GetEnv               = true
Environment          = "${dboard} ${save_failed_datafiles_env} ${fwklite_env}"
Copy_To_Spool        = false
Notification         = never
WhenToTransferOutput = On_Exit
on_exit_remove       = (ExitBySignal == FALSE && (ExitCode == 0 || (ExitCode == ${FAIL_JOB} && NumJobStarts>3))) || (NumJobStarts>6)
+IsFastQueueJob      = True
ImageSize            = $(($MEMORY_REQUIREMENTS*1024))
+DiskUsage           = $(($DISK_REQUIREMENTS*1024))
Requirements         = ${SITE_REQUIREMENTS}
# stop jobs from running if they blow up
periodic_hold        = ImageSize/1024 > ${HOLD_IMAGE_SIZE_FACTOR}*${MEMORY_REQUIREMENTS} || DiskUsage/1024 > ${HOLD_DISK_USAGE_FACTOR}*${DISK_REQUIREMENTS}
${ACCOUNTING_GROUP}
${REQUIRES_WHOLE_MACHINE_ATTR}
EOF


echo "Generating submit files in $runDir..."

#
# Loop over input files
#

EXCLUDE_ARG=
if [ "$EXCLUDE_INPUT_FILES" != "" ]; then
  EXCLUDE_ARG="-not -name $EXCLUDE_INPUT_FILES"
fi

find_command="find $LOCAL_INPUT_DIR -size +0 -name $MATCH_INPUT_FILES $EXCLUDE_ARG"

dbs_query() {
    query="find file where"
    for arg; do
      query="$query $arg"
    done
    python $DBSCMD_HOME/dbsCommandLine.py -c search --query="$query" | grep /store/
}
if [ "$INPUT_DBS_PATH" != "" ]; then
  query="dataset=$INPUT_DBS_PATH"
  if [ "$INPUT_RUNS" != "" ]; then
    query="$query and run in ($INPUT_RUNS)"
  fi
  find_command="dbs_query $query"
  check_input_file_existence=${ASSUME_INPUT_FILES_EXIST:-1}
  prepend_local_input_dir=1
fi

if [ "$INPUT_FILE_LIST" != "" ]; then
  find_command="cat $INPUT_FILE_LIST"
  check_input_file_existence=${ASSUME_INPUT_FILES_EXIST:-1}
  prepend_local_input_dir=1
fi

count=0
output_file_count=0
$find_command |
while read nextInputFile
do
    inputFileNames=""
    i=$INPUT_FILES_PER_JOB
    while [ $i -gt 0 ]; do
        nextInputFileOrigName="${nextInputFile}"
        if [ "$prepend_local_input_dir" = 1 ]; then
            nextInputFile="${LOCAL_INPUT_DIR}${nextInputFile}"
        fi

        if [ "$check_input_file_existence" = "1" ]; then
            if ! [ -f "$nextInputFile" ]; then
                echo "$nextInputFile does not exist, skipping"

                if [ "$SAVE_MISSING_INPUT_FILE_LIST" != "" ]; then
                    echo "$nextInputFileOrigName" >> "$SAVE_MISSING_INPUT_FILE_LIST"
                fi

		read nextInputFile || break
                continue
            fi
        fi

        if [ "$inputFileNames" = "" ]; then
            inputFileNames="\"${INPUT_BASE}/$nextInputFile\""
            firstInputFile="${nextInputFile}"
        else
            inputFileNames="${inputFileNames},\"${INPUT_BASE}/$nextInputFile\""
        fi

        i=$(($i-1))
        if [ $i -gt 0 ]; then
            read nextInputFile || break
        fi
    done
    [ "$inputFileNames" = "" ] && break

#
# Name the files
#
    output_file_count=$(($output_file_count+1))
    job_output_dir="$OUTPUT_DIR"
    if [ "$OUTPUT_FILES_PER_SUBDIR" != "0" ]; then
        job_output_dir="$job_output_dir/"$(($output_file_count/$OUTPUT_FILES_PER_SUBDIR+1))
    fi

    rootname=`basename $firstInputFile .root`
    jobtag=$runName-$rootname
    if [ ${#jobtag} -gt 245 ]; then
        # Condor (as of 6.9.4) cannot handle file names longer than 256
        jobtag=$rootname
    fi

    consub=$jobtag.sub
    conlog=$jobtag.log
    stdout=$jobtag.out
    stderr=$jobtag.err
    if [ "$FWKLITE" = 1 ]; then
      jobcfg=$jobtag.inputs
    else
      jobcfg=$jobtag.$CFG_EXTENSION
    fi
    outputFileName=$jobtag.root
    inputfiles="$jobcfg"
    if [ "$EXTRA_INPUTS" != "" ]; then
      inputfiles="$jobcfg,$EXTRA_INPUTS"
    fi

#
# Create and set to the job subdirectory
#

    cd $runDir || die "Failed to cd to $runDir"


    if [ "$SKIP_EXISTING_JOBS" = "1" ] && [ -d $jobtag ]; then
      continue
    fi

    if [ "$SKIP_EXISTING_OUTPUT" = "1" ]; then
      # Check for existing output file
      if outputFileExists $job_output_dir/$outputFileName; then
        continue
      fi
    fi

    count=$(($count+1))
    if [ ! -z $JOB_LIMIT ] && [ $count -gt $JOB_LIMIT ]; then
        echo "Job limit $JOB_LIMIT reached.  Halting creation of jobs."

        # eat up rest of input to avoid broken pipe message
        while read junk; do junk=""; done

        break
    fi
    echo -n "."

    mkdir -p $jobtag
    cd $jobtag || die "Failed to cd to $jobtag"

#
# Prepare job configuration file
#

    if [ "$FWKLITE" != 1 ]; then
      sed < $configTemplate \
          "s|\\\$inputFileNames|$inputFileNames|g;
           s|\\\$outputFileName|$outputFileName|g" > $jobcfg
    else
      # convert quoted comma-separated inputfile list to newline-separated
      echo "$inputFileNames" | sed \
        "s|\"||g;
         s|,|\n|g" > $jobcfg
    fi

#
# Prepare condor submit file for the job
#
    cat >> $submitFile <<EOF

InitialDir           = $jobtag
Arguments            = $jobcfg `basename $outputFileName` $job_output_dir
Transfer_Input_Files = $inputfiles
output               = $stdout
error                = $stderr
Log                  = $conlog
Queue
EOF
done || die

echo ""

cd $runDir

#
# Submit the job
#
if ! grep -q ^Queue $submitFile; then
  echo "No jobs were created, so there is nothing to do."
  rm $submitFile
  exit 0
elif [ -z "$NO_SUBMIT" ]; then
  # The job is messed up if X509_USER_PROXY is defined, because then
  # Condor doesn't override this value to point to the actual proxy
  # location on the execution node.
  unset X509_USER_PROXY

  # Convoluted way of sending a message to the screen
  # and to a log file about the sucess of submitting
  # jobs and then "die"ing if needed.
  condor_submit $submitFile >> $farmoutLogFile
  if [ $? -eq 1 ]; then
    echo "Failed to submit $submitFile" >> $farmoutLogFile
    die 
  else 
    cat $farmoutLogFile
    echo "Date: "`date` >> $farmoutLogFile
  fi
else
  echo "Submit file $submitFile has been created but not submitted."
fi

echo -n "Jobs for $jobName are created in "
pwd
cd $originalDir


echo ""
echo "Your jobs should show up in ~6 minutes"
echo "at the NEW job monitoring web page :"
echo "    http://www.hep.wisc.edu/cms/comp/jobs/"


